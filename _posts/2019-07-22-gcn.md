---
layout: post
title:  "How to deal the Graph data in Deep learning with Graph Convolution Netwoks(GCN) "
date: 2019-07-22
comments: True
mathjax: True
---
<ol>
 <li>Introduction to Graph Convolution Networks(Why GCN?)</li>
 <li> A Brief History of GCN </li>
 <li>Defination-What is graph?</li>
 <li>What GCN does?</li>
 </ol>
 
 <a href="https://blog.bayeslabs.co/2019/07/04/Generating-Molecules-using-Char-RNN-in-Pytorch.html">In previous post</a>.
 I have explained about Generation of molecues using SMILE Dataset. But I want to explore the things if we have to work on Graph dataset.
 
 SMILES strings are generated from a graph-based representation of molecules, thereby working in the original graph space has the benefit of 
 removing additional overhead. With recent progress in the area of deep learning on graphs, training deep generative models directly on 
 graph representations becomes a feasible alternative that has been explored in a range of recent works.
 
 <b>Why GCN-</b>
 Recently, there is increasing interest in extending deep learning approaches for graph data. Driven by the success of deep learning, researchers have borrowed ideas from convolution networks, 
 recurrent networks, and deep autoencoders to design the architecture of graph neural networks.
While deep learning has achieved great success on Euclidean data, there is an increasing number of applications where data are generated from the
non-Euclidean domain and need to be effectively analyzed. For instance, in e-commerce, a graph-based learning system is able to exploit the
interactions between users and products to make highly accurate recommendations.
In chemistry, molecules are modeled as graphs and their bio-activity needs to be identified for drug discovery. User data on social networks,
gene data on biological regulatory networks, log data on telecommunication networks, or text documents on word embeddings are important examples 
of data lying on irregular or non-Euclidean domains that can be structured with graphs. which are universal representations of heterogeneous pair wise relationships. 
Graphs can encode complex geometric structures and can be studied with strong mathematical tools such as spectral graph theory (<a href="https://arxiv.org/pdf/1901.00596.pdf"></a>). 
The complexity of graph data has imposed significant challenges on existing machine learning algorithms. This is because graph data are irregular. Each graph has a variable size of 
unordered nodes and each node in a graph has a different number of neighbors, causing some important operations (e.g., convolutions), 
which are easy to compute in the image domain but are not directly applicable to the graph domain anymore.
To handle the complexity of graph data, new generalizations and definitions for important operations have been rapidly developed over the past few years. 
For instance, Below Figure illustrates how a kind of graph convolution is inspired by a standard 2D convolution. This survey aims to provide a comprehensive overview of these methods, 
for both interested researchers who want to enter this rapidly developing field and experts who would like to compare graph neural network algorithms.
{%include image.html url="/assets/img/GCN-1.jpg" description="Fig- 2D Convolution vs. Graph Convolution." %}
(a) 2D Convolution. Analogous to a graph, each pixel in an image is taken as a node where neighbors are determined by the filter size. 
The 2D convolution takes a weighted average of pixel values of the red node along with its neighbors. The neighbors of a node are ordered and have a fixed size.
(b) Graph Convolution. To get a hidden representation of the red node, one simple solution of graph convolution operation takes the average value of node features 
of the red node along with its neighbors. Different from image data, the neighbors of a node are unordered and variable in size.

<b>A Brief History of Graph Convolutional Networks-</b>
GCNs are a very powerful neural network architecture for machine learning on graphs.This method directly perform the convolution in the graph domain by aggregating the neighbor nodes' information. 
Together with sampling strategies, the computation can be performed in a batch of nodes instead of the whole graph, which has the potential to improve efficiency. 
In addition to graph convolutional networks, many alternative graph neural networks have been developed in the past few years. 
These approaches include graph attention networks, graph auto encoders, graph generative networks, and graph spatial-temporal networks.

<b>Graph neural networks vs. network embedding -</b> The research on graph neural networks is closely related to graph embedding or network embedding,Network embedding aims to represent network vertices into a low-dimensional vector space, by preserving both network topology structure and node content information,so that any subsequent graph analytics tasks such as classification, clustering, and recommendation can be easily performed by using simple off-the-shelf machine learning algorithm (eg.support vector machines for classification).Many network embedding algorithms are typically unsupervised algorithms and they can be broadly classified into three groups ex-, matrix factorization ,random walks ,and deep learning approaches.
The deep learning approaches for network embedding at the same time belong to graph neural networks, 
which include graph autoencoder-based algorithms (e.g., DNGR and SDNE ) and graph convolution neural networks with unsupervised training(e.g., GraphSage ).

<b> DEFINITION –What is Graph-</b>
A graph $G$ can be well described by the set of vertices $V$ and $E$.
<b> $$G=(V,E)$$,where $V$ is the set of nodes, $E$ is the set of edges.</b>
Edges can be either directed or undirected, depending on whether there exist directional dependencies between vertices,       e<sub>i,j</sub>= (v<sub>i</sub>, v<sub>j</sub>),∈ E to denote an edge.
The degree of a node is the number of edges connected to it.The vertices are often called nodes, let v<sub>i</sub> ∈ V to denote a node.

a GCN takes as input-

An input feature matrix X<sub>i</sub>  for every node i; summarized in a $N×D$ feature matrix $X$, ($N$: number of nodes, $D$: number of input features)
A description of the graph structure in matrix form; typically in the form of an adjacency matrix $A$ (or some function thereof).The adjacency matrix $A$ of size $N×N$ ,where  A<sub>i,j</sub>= 1 if there is an edge from vertex $v<sub>i</sub>$ to vertex $v<sub>j</sub>$ , and A<sub>i,j</sub> = 0  otherwise.In this case, we say that vertex $v<sub>i</sub>$ has position $i$ in $A$. Moreover, if $A<sub>i,j</sub> = 1$ we say $v<sub>i</sub>$ and $v<sub>j</sub>$ are adjacent.

Every neural network layer can then be written as a non-linear function
                       $$ H <sup>(l+1)</sup>=f(H<sup>(l)</sup>,A)$$
with $H(0)=X$ and $H(l)=Z$ (or $Z$ for graph-level outputs), $l$ being the number of layers, $f$ is a propagation rule.Each layer 
$H <sub>(l)</sub>$ corresponds to an $N ×D<sup>(l)</sup>$ feature matrix where each row is a feature representation of a node.
As any other stacked neural layers, GCN can be multiple layers. For each layer,
                                   $H<sup>(l+1)</sup>=f(H<sup>(l)</sup>,A),$
The input matrix $A$ is the adjacency matrix. If there is a  connection between two nodes, then the element is $1$, otherwise 0.
H<sup>(l)</sup> is the graph-level outputs of layer $l$. In other words, to generate the output for the next layer, we take the current layer as well as the adjacency matrix, then apply a non-linear function $f$.
The final output of GCN at the laster layer is a matrix $Z$, and the shape is $N*F$. $N$ is the number of nodes, $F$ is the number of output features per node.
 Initially, we need $H(0)  =X, X$ here is a matrix with node features with the shape being $N*D$, and $D$ is the number of input features.
So to start the GCN, we need to provide $A$ and $X$, eventually the output is $Z$ which is able to provide meaningful embedding for each node in the graph.
When choosing the function, a simple way is to apply a ReLu and we add a weight matrix W to each layer, then the function becomes:
H<sup>(l+1)</sup> = σ (AH<sup>(l)</up>W<sup>(l)</sup>)

At each layer, these features are aggregated to form the next layer's features using the propagation rule $f$. In this way, features become increasingly more abstract at each consecutive layer.Where, $W<sup>(l)</sup>$ is a weight matrix for the Lth neural network layer and $σ(⋅) $ is a non-linear activation function like the ReLU. The weight matrix has dimensions $F<sup>l</sup> ×F<sup>l+1</sup>$ ; in other words the size of the second dimension of the weight matrix determines the number of features at the next layer.
However, simply by multiplying with adjacency matrix $A$ may lead to two problems.The first one is the adjacency matrix is not considering itself for each node unless it is self-connected, multiplication with $A$ means that, for every node, we sum up all the feature vectors of all neighboring nodes but not the node itself (unless there are self-loops in the graph). In this case, we force the diagonal elements to be $1$, or add an identity matrix $I$ into $A$. 
$A<sup>^</sup> =A+I$
Since the node is now a neighbor of itself, the node's own features is included when summing up the features of its neighbors!

