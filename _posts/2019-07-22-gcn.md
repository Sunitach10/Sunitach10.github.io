---
layout: post
title:  "How to deal the Graph data in Deep learning with Graph Convolution Netwoks(GCN) "
date: 2019-07-22
comments: True
mathjax: True
---
<ol>
 <li>Introduction to Graph Convolution Networks(Why GCN?)</li>
 <li> A Brief History of GCN </li>
 <li>Defination-What is graph?</li>
 <li>What GCN does?</li>
 </ol>
 
 <a href="https://blog.bayeslabs.co/2019/07/04/Generating-Molecules-using-Char-RNN-in-Pytorch.html">In previous post</a>.
 I have explained about Generation of molecues using SMILE Dataset. But I want to explore the things if we have to work on Graph dataset.
 
 SMILES strings are generated from a graph-based representation of molecules, thereby working in the original graph space has the benefit of 
 removing additional overhead. With recent progress in the area of deep learning on graphs, training deep generative models directly on 
 graph representations becomes a feasible alternative that has been explored in a range of recent works.
 
 <b>Why GCN-</b>
 Recently, there is increasing interest in extending deep learning approaches for graph data. Driven by the success of deep learning, researchers have borrowed ideas from convolution networks, 
 recurrent networks, and deep autoencoders to design the architecture of graph neural networks.
While deep learning has achieved great success on Euclidean data, there is an increasing number of applications where data are generated from the
non-Euclidean domain and need to be effectively analyzed. For instance, in e-commerce, a graph-based learning system is able to exploit the
interactions between users and products to make highly accurate recommendations.
In chemistry, molecules are modeled as graphs and their bio-activity needs to be identified for drug discovery. User data on social networks,
gene data on biological regulatory networks, log data on telecommunication networks, or text documents on word embeddings are important examples 
of data lying on irregular or non-Euclidean domains that can be structured with graphs. which are universal representations of heterogeneous pair wise relationships. 
Graphs can encode complex geometric structures and can be studied with strong mathematical tools such as spectral graph theory (<a href="https://arxiv.org/pdf/1901.00596.pdf"></a>). 
The complexity of graph data has imposed significant challenges on existing machine learning algorithms. This is because graph data are irregular. Each graph has a variable size of 
unordered nodes and each node in a graph has a different number of neighbors, causing some important operations (e.g., convolutions), 
which are easy to compute in the image domain but are not directly applicable to the graph domain anymore.
To handle the complexity of graph data, new generalizations and definitions for important operations have been rapidly developed over the past few years. 
For instance, Below Figure illustrates how a kind of graph convolution is inspired by a standard 2D convolution. This survey aims to provide a comprehensive overview of these methods, 
for both interested researchers who want to enter this rapidly developing field and experts who would like to compare graph neural network algorithms.
{%include image.html url="/assets/img/GCN-1.jpg" description="Fig- 2D Convolution vs. Graph Convolution." %}
(a) 2D Convolution. Analogous to a graph, each pixel in an image is taken as a node where neighbors are determined by the filter size. 
The 2D convolution takes a weighted average of pixel values of the red node along with its neighbors. The neighbors of a node are ordered and have a fixed size.
(b) Graph Convolution. To get a hidden representation of the red node, one simple solution of graph convolution operation takes the average value of node features 
of the red node along with its neighbors. Different from image data, the neighbors of a node are unordered and variable in size.

<b>A Brief History of Graph Convolutional Networks-</b>
GCNs are a very powerful neural network architecture for machine learning on graphs.This method directly perform the convolution in the graph domain by aggregating the neighbor nodes' information. 
Together with sampling strategies, the computation can be performed in a batch of nodes instead of the whole graph, which has the potential to improve efficiency. 
In addition to graph convolutional networks, many alternative graph neural networks have been developed in the past few years. 
These approaches include graph attention networks, graph auto encoders, graph generative networks, and graph spatial-temporal networks.

<b>Graph neural networks vs. network embedding -</b> The research on graph neural networks is closely related to graph embedding or network embedding,Network embedding aims to represent network vertices into a low-dimensional vector space, by preserving both network topology structure and node content information,so that any subsequent graph analytics tasks such as classification, clustering, and recommendation can be easily performed by using simple off-the-shelf machine learning algorithm (eg.support vector machines for classification).Many network embedding algorithms are typically unsupervised algorithms and they can be broadly classified into three groups ex-, matrix factorization ,random walks ,and deep learning approaches.
The deep learning approaches for network embedding at the same time belong to graph neural networks, 
which include graph autoencoder-based algorithms (e.g., DNGR and SDNE ) and graph convolution neural networks with unsupervised training(e.g., GraphSage ).

<b> DEFINITION –What is Graph-</b>
A graph $G$ can be well described by the set of vertices $V$ and $E$.
<b> $$G=(V,E)$$,where $V$ is the set of nodes, $E$ is the set of edges.</b>
Edges can be either directed or undirected, depending on whether there exist directional dependencies between vertices, $eij = (vi , vj ) ∈ E $to denote an edge .
The degree of a node is the number of edges connected to it.The vertices are often called nodes, let $vi ∈ V$ to denote a node.
a GCN takes as input-
An input feature matrix $xi,$ for every node $i$; summarized in a $N×D$ feature matrix $X$, ($N$: number of nodes, $D$: number of input features)
A description of the graph structure in matrix form; typically in the form of an adjacency matrix $A$ (or some function thereof).The adjacency matrix $A$ of size $N×N$ ,where $ A<sub>i,j</sub>= 1 $if there is an edge from vertex $v<sub>i</sub>$ to vertex $v<sub>j</sub> , and A<sub>i,j</sub> = 0 $ otherwise.In this case, we say that vertex $v<sub>i</sub>$ has position $i$ in A. Moreover, if A<sub>i,j</sub> = 1 we say v<sub>i</sub> and v<sub>j</sub> are adjacent

Every neural network layer can then be written as a non-linear function
                       $$ H <sup>(l+1)</sup>=f(H<sup>(l)</sup>,A)$$

