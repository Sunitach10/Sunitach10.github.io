---
layout: post
title:  "MolGAN: Generative model for molecular graphs "
date: 2019-07-04
comments: True
mathjax: True
---
<b>Deep generative models</b> for graph-structured data offer a new angle on the problem of chemical synthesis: by optimizing differentiable 
models that directly generate molecular graphs,I am writing about MolGAN, an implicit, likelihood free generative model for small molecular graphs .Here we will use generative adversarial networks (GANs) to operate directly on graph-structured data.
We combine our approach with a reinforcement learning objective to encourage the generation of molecules with specific desired/particular 
chemical properties.

<b>Dataset-</b> In implementation we used the QM9 chemical database, we found that our model is capable of generating close to 100% valid 
compounds.and we used RDKIT library.

<b>Introduction – </b>Generating new chemical compounds with desired properties is a challenging task with important applications such as
(Schneider & Fechner, 2005) Drug Discovery.Most works in this area make use of a so-called SMILES representation(what is smile representation of molecules.
<a href="https://github.com/bayeslabs/bayeslabs.github.io/blob/master/_posts/2019-07-04-Generating-Molecules-using-Char-RNN-in-Pytorch.md">I have explained in my previous blog</a>)
: a string-based representation derived from molecular graphs.
String-based representations of molecules, however, have certain disadvantages: RNNs have to spend capacity on learning both the syntactic rules and the order ambiguity of the representation.
Our molecular GAN (MolGAN) model (outlined in Figure 1) is to address the generation of graph-structured data in the context of molecular synthesis using GANs. 
The generative model of MolGAN predicts discrete graph structure at once (i.e., non sequentially) for computational efficiency, although sequential variants are possible in general. MolGAN further utilizes a permutation-invariant discriminator and reward network (for RL-based optimization towards desired chemical properties) based on graphconvolutionlayers(GCN) link............that both operate directly on graph structured representations.
{%include image.html url="/assets/img/molgan-1.png" description="Fig-1 MolGAN" %}

A vector z is sampled from a prior and passed to the generator which outputs the graph representation of a molecule. The discriminator classiﬁes whether the molecular graph comes from the generator or the dataset. The reward network tries to estimate the reward for the chemical properties of a particular molecule provided by an external software.

Our goal is-obtaining a generator that can output valid molecular structures with good properties.

<b>Molecules as graphs -</b> The SMILES syntax, is not robust to small changes or mistakes, which can result in the generation of invalid or drastically different structures. We consider that each molecule can be represented by an undirected graph G with a set of edges E and nodes V. Each atom corresponds to a node vi ∈ V that is associated with a T-dimensional one-hot vector X<sub>i</sub> , indicating the type of the atom. We further represent each atomic bond as an edge (vi , vj ) ∈ E associated with a bond type y ∈ {1, …, Y }. For a molecular graph with N nodes, we can summarize this representation in a node feature matrix X = [X<sub>1</sub>,..., X<sub>N</sub>],T ∈ R<sup>N×T</sup> and an adjacency tensor A ∈ R<sup>N×N×Y</sup> where A<sub>ij</sub> ∈ R<sup>Y</sup>is a one-hot vector indicating the type of the edge between i and j.

<b>Generative adversarial networks -</b> GANs are implicit generative models in the sense that they allow for inference of model parameters without requiring one to specify a likelihood. 
A GAN consist of <b>two main components</b>: a generative model G<sub>(θ)</sub> , that learns a map from a prior to the data distribution to sample new data-points, and a discriminative model Dφ, that learns to classify whether samples came from the data distribution rather than from G<sub>(θ)</sub>.Those two models are implemented as neural networks and trained simultaneously with stochastic gradient descent (SGD).G<sub>(θ)</sub> and Dφ have different objectives, and they can be seen as two players in a minimax game min θ max φ Ex∼P<sub>(data)</sub> (x) [log Dφ(x)]+ Ez∼pz(z) [log(1 − Dφ(Gθ(z))] , where G(θ) tries to generate samples to fool the discriminator and Dφ tries to differentiate samples correctly.
To prevent undesired behaviour such as mode collapse (Salimans et al., 2016) and to stabilize learning, we can use minibatch discrimination and improved WGAN an alternative and more stable GAN model that minimizes a better suited divergence.
