---
layout: post
title:  "MolGAN: Generative model for molecular graphs "
date: 2019-07-04
comments: True
mathjax: True
---
<b>Deep generative models</b> for graph-structured data offer a new angle on the problem of chemical synthesis: by optimizing differentiable 
models that directly generate molecular graphs,I am writing about MolGAN, an implicit, likelihood free generative model for small molecular graphs .Here we will use generative adversarial networks (GANs) to operate directly on graph-structured data.
We combine our approach with a reinforcement learning objective to encourage the generation of molecules with specific desired/particular 
chemical properties.

<b>Dataset-</b> In implementation we used the QM9 chemical database, we found that our model is capable of generating close to 100% valid 
compounds.and we used RDKIT library.

<b>Introduction – </b>Generating new chemical compounds with desired properties is a challenging task with important applications such as
(Schneider & Fechner, 2005) Drug Discovery.Most works in this area make use of a so-called SMILES representation(what is smile representation of molecules.
<a href="https://github.com/bayeslabs/bayeslabs.github.io/blob/master/_posts/2019-07-04-Generating-Molecules-using-Char-RNN-in-Pytorch.md">I have explained in my previous blog</a>)
: a string-based representation derived from molecular graphs.
String-based representations of molecules, however, have certain disadvantages: RNNs have to spend capacity on learning both the syntactic rules and the order ambiguity of the representation.
Our molecular GAN (MolGAN) model (outlined in Figure 1) is to address the generation of graph-structured data in the context of molecular synthesis using GANs. 
The generative model of MolGAN predicts discrete graph structure at once (i.e., non sequentially) for computational efficiency, although sequential variants are possible in general. MolGAN further utilizes a permutation-invariant discriminator and reward network (for RL-based optimization towards desired chemical properties) based on graphconvolutionlayers(GCN) link............that both operate directly on graph structured representations.
{%include image.html url="/assets/img/molgan-1.png" description="Fig-1 MolGAN" %}

A vector z is sampled from a prior and passed to the generator which outputs the graph representation of a molecule. The discriminator classiﬁes whether the molecular graph comes from the generator or the dataset. The reward network tries to estimate the reward for the chemical properties of a particular molecule provided by an external software.

Our goal is-obtaining a generator that can output valid molecular structures with good properties.

<b>Molecules as graphs -</b> The SMILES syntax, is not robust to small changes or mistakes, which can result in the generation of invalid or drastically different structures. We consider that each molecule can be represented by an undirected graph G with a set of edges E and nodes V. Each atom corresponds to a node vi ∈ V that is associated with a T-dimensional one-hot vector X<sub>i</sub> , indicating the type of the atom. We further represent each atomic bond as an edge (vi , vj ) ∈ E associated with a bond type y ∈ {1, …, Y }. For a molecular graph with N nodes, we can summarize this representation in a node feature matrix X = [X<sub>1</sub>,..., X<sub>N</sub>],T ∈ R<sup>N×T</sup> and an adjacency tensor A ∈ R<sup>N×N×Y</sup> where A<sub>ij</sub> ∈ R<sup>Y</sup>is a one-hot vector indicating the type of the edge between i and j.

<b>Generative adversarial networks -</b> GANs are implicit generative models in the sense that they allow for inference of model parameters without requiring one to specify a likelihood. 
A GAN consist of <b>two main components</b>: a generative model G<sub>(θ)</sub> , that learns a map from a prior to the data distribution to sample new data-points, and a discriminative model Dφ, that learns to classify whether samples came from the data distribution rather than from G<sub>(θ)</sub>.Those two models are implemented as neural networks and trained simultaneously with stochastic gradient descent (SGD).G<sub>(θ)</sub> and Dφ have different objectives, and they can be seen as two players in a minimax game min θ max φ Ex∼P<sub>(data)</sub> (x) [log Dφ(x)]+ E<sub>z</sub>∼P<sub>z</sub>(z) [log(1 − Dφ(G<sub>(θ)</sub> (z))] , where G<sub>(θ)</sub>  tries to generate samples to fool the discriminator and Dφ tries to differentiate samples correctly.
To prevent undesired behaviour such as mode collapse (Salimans et al., 2016) and to stabilize learning, we can use minibatch discrimination and improved WGAN an alternative and more stable GAN model that minimizes a better suited divergence.

<b>Reinforcement Learning for Molecular design-</b>

A GAN generator learns a transformation from a prior distribution to the data distribution.Thus, generated samples resemble data samples. However, in drug design methods, we are not only interested in generating chemically valid compounds, but we want them to have some useful property (e.g., to be easily synthesizable). Therefore, we also optimize the generation process towards some non-differentiable metrics using reinforcement learning.
<b>Deterministic policy gradients</b>-Policy Gradient methods update the Probability distribution of actions so that actions with higher expected reward have a higher Probability value for an observed value.
A Policy can be either Deterministic or Stochastic. A <b>Deterministic Policy</b> is policy that maps state to actions.you give it a state and the function returns an action to take.a deterministic policy is represented by µθ(s) = a which deterministically outputs an action A. <b>Stochastic Policy</b>-outputs a Probability distribution over actions.This is represented by πθ(s) = pθ(a|s) which is a parametric probability distribution in θ that selects a categorical action a conditioned on an environmental state s.REINFORCE in combination with a stochastic policy that models graph generation as a set of categorical choices (actions). However, we found that it converged poorly due to the high dimensional action space when generating graphs at once. We instead base our method on a deterministic policy gradient algorithm which is known to perform well in high-dimensional action spaces .In our case

<b>Action</b>-Generation of Molecule,

Environment/Reward-Biochemical Evaluation of Molecule

<b>Policy</b>-Generative Model

<b>Model -</b> The MolGAN architecture consists of three main components: a generator G<sub>(θ)</sub>, a discriminator Dφ and a reward network Rˆ<sub> ψ</sub>.
{%include image.html url="/assets/img/molgan-2.jpg" %}
From left: the generator takes a sample from a prior distribution and generates a dense adjacency tensor A and an annotation matrix X. Subsequently, sparse and discrete ˜ A and ˜ X are obtained from A and X respectively via categorical sampling. The combination of ˜ A and ˜ X represents an annotated molecular graph which corresponds to a speciﬁc chemical compound. Finally, the graph is processed by both the discriminator and reward networks that are invariant to node order permutations and based on Relational-GCN layers.
